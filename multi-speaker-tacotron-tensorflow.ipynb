{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32b330ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: '-H'\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "ModuleNotFoundError: No module named 'nltk'\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt\n",
    "!python3 -c \"import nltk; nltk.download('punkt')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2408af25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jwj71\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk;\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5acd7a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 3.6.13 :: Anaconda, Inc.\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56c2c8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download news video+text:   0%|                                                                | 0/946 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "download_news_video_and_content() got an unexpected keyword argument 'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\users\\jwj71\\anaconda3\\envs\\tf_test\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\nTypeError: download_news_video_and_content() got an unexpected keyword argument 'encoding'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\GitHub\\multi-speaker-tacotron-tensorflow\\datasets\\son\\download.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     results = parallel_run(\n\u001b[1;32m--> 149\u001b[1;33m             fn, news_ids, desc=\"Download news video+text\", parallel=True)\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\multi-speaker-tacotron-tensorflow\\utils\\__init__.py\u001b[0m in \u001b[0;36mparallel_run\u001b[1;34m(fn, items, desc, parallel)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             for out in tqdm(pool.imap_unordered(\n\u001b[1;32m--> 147\u001b[1;33m                     fn, items), total=len(items), desc=desc):\n\u001b[0m\u001b[0;32m    148\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jwj71\\anaconda3\\envs\\tf_test\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    947\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jwj71\\anaconda3\\envs\\tf_test\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    733\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: download_news_video_and_content() got an unexpected keyword argument 'encoding'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import m3u8\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "from functools import partial\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from utils import get_encoder_name, parallel_run, makedirs\n",
    "\n",
    "API_URL = 'http://api.jtbc.joins.com/ad/pre/NV10173083'\n",
    "BASE_URL = 'http://nsvc.jtbc.joins.com/API/News/Newapp/Default.aspx'\n",
    "\n",
    "def soupify(text):\n",
    "    return BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "def get_news_ids(page_id):\n",
    "    params = {\n",
    "        'NJC': 'NJC300',\n",
    "        'CAID': 'NC10011174',\n",
    "        'PGI': page_id,\n",
    "    }\n",
    "\n",
    "    response = requests.request(\n",
    "        method='GET', url=BASE_URL, params=params,\n",
    "    )\n",
    "    soup = soupify(response.text)\n",
    "\n",
    "    return [item.text for item in soup.find_all('news_id')]\n",
    "\n",
    "def download_news_video_and_content(\n",
    "        news_id, base_dir, chunk_size=32*1024,\n",
    "        video_dir=\"video\", asset_dir=\"assets\", audio_dir=\"audio\"):\n",
    "\n",
    "    video_dir = os.path.join(base_dir, video_dir)\n",
    "    asset_dir = os.path.join(base_dir, asset_dir)\n",
    "    audio_dir = os.path.join(base_dir, audio_dir)\n",
    "\n",
    "    makedirs(video_dir)\n",
    "    makedirs(asset_dir)\n",
    "    makedirs(audio_dir)\n",
    "\n",
    "    text_path = os.path.join(asset_dir, \"{}.txt\".format(news_id))\n",
    "    original_text_path = os.path.join(asset_dir, \"original-{}.txt\".format(news_id))\n",
    "\n",
    "    video_path = os.path.join(video_dir, \"{}.ts\".format(news_id))\n",
    "    audio_path = os.path.join(audio_dir, \"{}.wav\".format(news_id))\n",
    "\n",
    "    params = {\n",
    "        'NJC': 'NJC400',\n",
    "        'NID': news_id, # NB11515152\n",
    "        'CD': 'A0100',\n",
    "    }\n",
    "\n",
    "    response = requests.request(\n",
    "        method='GET', url=BASE_URL, params=params,\n",
    "    )\n",
    "    soup = soupify(response.text)\n",
    "\n",
    "    article_contents = soup.find_all('article_contents')\n",
    "\n",
    "    assert len(article_contents) == 1, \\\n",
    "            \"# of <article_contents> of {} should be 1: {}\".format(news_id, response.text)\n",
    "\n",
    "    text = soupify(article_contents[0].text).get_text() # remove <div>\n",
    "\n",
    "    with open(original_text_path, \"w\",encoding='UTF-8') as f:\n",
    "        f.write(text,encoding='UTF-8')\n",
    "\n",
    "    with open(text_path, \"w\",encoding='UTF-8') as f:\n",
    "        from nltk import sent_tokenize\n",
    "\n",
    "        text = re.sub(r'\\[.{0,80} :\\s.+]', '', text) # remove quote\n",
    "        text = re.sub(r'☞.+http.+\\)', '', text) # remove quote\n",
    "        text = re.sub(r'\\(https?:\\/\\/.*[\\r\\n]*\\)', '', text) # remove url\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        sentences = [sent for sentence in sentences for sent in sentence.split('\\n') if sent]\n",
    "\n",
    "        new_texts = []\n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            sent = re.sub(r'\\([^)]*\\)', '', sent)\n",
    "            #sent = re.sub(r'\\<.{0,80}\\>', '', sent)\n",
    "            sent = sent.replace('…', '.')\n",
    "            new_texts.append(sent)\n",
    "\n",
    "        f.write(\"\\n\".join([sent for sent in new_texts if sent]),encoding='UTF-8')\n",
    "\n",
    "    vod_paths = soup.find_all('vod_path')\n",
    "\n",
    "    assert len(vod_paths) == 1, \\\n",
    "            \"# of <vod_path> of {} should be 1: {}\".format(news_id, response.text)\n",
    "\n",
    "    if not os.path.exists(video_path):\n",
    "        redirect_url = soup.find_all('vod_path')[0].text\n",
    "\n",
    "        list_url = m3u8.load(redirect_url).playlists[0].absolute_uri\n",
    "        video_urls = [segment.absolute_uri for segment in m3u8.load(list_url).segments]\n",
    "\n",
    "        with open(video_path, \"wb\") as f:\n",
    "            for url in video_urls:\n",
    "                response = requests.get(url, stream=True)\n",
    "                total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "                for chunk in response.iter_content(chunk_size):\n",
    "                    if chunk: # filter out keep-alive new chunks\n",
    "                        f.write(chunk,encoding='UTF-8')\n",
    "\n",
    "    if not os.path.exists(audio_path):\n",
    "        encoder = get_encoder_name()\n",
    "        command = \"{} -y -loglevel panic -i {} -ab 160k -ac 2 -ar 44100 -vn {}\".\\\n",
    "                format(encoder, video_path, audio_path)\n",
    "        subprocess.call(command, shell=True)\n",
    "\n",
    "    return True\n",
    "\n",
    "__file__ = \"C:\\\\Users\\\\jwj71\\\\Documents\\\\GitHub\\\\multi-speaker-tacotron-tensorflow\\\\datasets\\\\son\\\\download.py\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    news_ids = []\n",
    "    page_idx = 1\n",
    "\n",
    "    base_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    news_id_path = os.path.join(base_dir, \"news_ids.json\")\n",
    "\n",
    "    if not os.path.exists(news_id_path):\n",
    "        while True:\n",
    "            tmp_ids = get_news_ids(page_idx)\n",
    "            if len(tmp_ids) == 0:\n",
    "                break\n",
    "\n",
    "            news_ids.extend(tmp_ids)\n",
    "            print(\" [*] Download page {}: {}/{}\".format(page_idx, len(tmp_ids), len(news_ids)))\n",
    "\n",
    "            page_idx += 1\n",
    "\n",
    "        with open(news_id_path, \"w\",encoding='UTF-8') as f:\n",
    "            json.dump(news_ids, f, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        with open(news_id_path,encoding='UTF-8') as f:\n",
    "            news_ids = json.loads(f.read())\n",
    "\n",
    "    exceptions = [\"NB10830162\"]\n",
    "    news_ids = list(set(news_ids) - set(exceptions))\n",
    "\n",
    "    fn = partial(download_news_video_and_content, base_dir=base_dir,encoding='UTF-8')\n",
    "\n",
    "    results = parallel_run(\n",
    "            fn, news_ids, desc=\"Download news video+text\", parallel=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce82c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m datasets.generate_data ./datasets/moon/alignment.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "becccea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7ce2e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jwj71\\Documents\\GitHub\\multi-speaker-tacotron-tensorflow\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc46d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
